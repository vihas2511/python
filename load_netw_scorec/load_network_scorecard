'''Function loading the carrier dashboard tables
'''

import pyspark.sql.functions as f
from pyspark.sql import Window
import utils
from get_src_data import get_transfix as tvb
from load_netw_scorec import expr_network_scorecard as expr


def get_weekly_network_sccrd_star(
        logging, spark_session, trans_vsblt_db_name, target_db_name, staging_location, debug_mode_ind, debug_postfix):


    tac_tender_pg_summary_df = \
        tvb.get_tac_tender_pg_summary_from_view(logging, spark_session, target_db_name, target_db_name, staging_location,
                                      debug_mode_ind, debug_postfix) \
        .withColumn("carr_num", f.expr(expr.tendigit_carrier_id_expr))\
        .withColumnRenamed("carr_num", "long_forward_agent_id") \
        .withColumnRenamed("calendar_year_week_tac", "week_year_val") \
        .withColumnRenamed("carrier_id", "forward_agent_id") \
        .withColumn("lane_key", f.concat(f.col("origin_zone_ship_from_code"), f.lit("-"), f.col("dest_ship_from_code")))\
        .withColumnRenamed("tms_service_code", "service_tms_code") \


    monster_data_df = \
        tvb.get_monster_view_data(logging, spark_session, target_db_name, target_db_name, staging_location, debug_mode_ind,
                          debug_postfix)\
        .withColumn("lane_name", f.concat(f.col("parent_loc_code"), f.lit("-"), f.col("dest_ship_from_code")))\
        .withColumnRenamed("ship_week_num", "week_year_val") \

    lot_data_df = \
        tvb.get_lot_view_data(logging, spark_session, target_db_name, target_db_name, staging_location, debug_mode_ind,
                          debug_postfix)\
        .withColumn("lane_name", f.concat(f.col("origin_zone_code"), f.lit("-"), f.col("dest_ship_from_code")))\
        .withColumnRenamed("ship_week_num", "week_year_val")

    vfr_data_df = \
        tvb.get_vfr_agg_data(logging, spark_session, target_db_name, target_db_name, staging_location,
                              debug_mode_ind, debug_postfix) \
        .withColumn("lane_name", f.concat(f.col("origin_zone_ship_from_code"), f.lit("-"), f.col("dest_loc_code"))) \
        .withColumn("carrier_id", f.regexp_replace(f.col('carr_id'), '^0*', ''))\
        .withColumn("formatted_actl_ship_date", f.concat(f.substring(f.col("shpmt_start_date"), 7, 4), f.lit("-"),
                                                        f.substring(f.col("shpmt_start_date"), 4, 2), f.lit("-"),
                                                        f.substring(f.col("shpmt_start_date"), 1, 2))) \
        .withColumn("calendar_week_num", f.lpad(f.weekofyear("formatted_actl_ship_date"), 2, '0')) \
        .withColumn("calendar_year_num", f.year("formatted_actl_ship_date")) \
        .withColumn("week_year_val", f.concat(f.col("calendar_week_num"), f.lit("/"), f.col("calendar_year_num")))


        
    logging.info("Calculating MONSTER aggregated data.")

    monster_calc_df = \
        monster_data_df.select("dest_ship_from_code", "ship_to_party_desc", "carr_num", 
                               "carr_desc", "origin_zone_ship_from_code", "actual_ship_date", "lane_name",
                               "service_tms_code", "shpmt_cnt", "shpmt_on_time_cnt", "measrbl_shpmt_cnt").distinct() \
        .groupBy("dest_ship_from_code", "ship_to_party_desc", "carr_num", "carr_desc", "origin_zone_ship_from_code",
                 "actual_ship_date", "lane_name", "service_tms_code") \
        .agg(f.sumDistinct("shpmt_cnt").alias("shpmt_cnt_sum"),
             f.sumDistinct("shpmt_on_time_cnt").alias("shpmt_on_time_cnt_sum"),
             f.sumDistinct("measrbl_shpmt_cnt").alias("measrbl_shpmt_cnt_sum"))

    logging.info("Calculating MONSTER aggregated data has finished.")

    utils.manageOutput(logging, spark_session, monster_calc_df, 0, "monster_calc_df", target_db_name,
                       staging_location, debug_mode_ind, "_{}{}".format(target_db_name, debug_postfix))

    logging.info("Joining data (1).")


    netw_scorec1_df = monster_data_df.drop("shpmt_cnt").drop("shpmt_on_time_cnt")\
        .drop("measrbl_shpmt_cnt")\
        .join(monster_calc_df, ["dest_ship_from_code", "ship_to_party_desc", "carr_num", "carr_desc",
                             "origin_zone_ship_from_code", "actual_ship_date", "lane_name", "service_tms_code"]) \
        .withColumnRenamed("carr_num", "forward_agent_id") \
        .groupBy("ship_to_party_code", "dest_ship_from_code", "forward_agent_id",
                 "origin_zone_ship_from_code", "service_tms_code", "lane_name", "week_year_val") \
        .agg(f.max("ship_to_party_desc").alias("ship_to_party_desc_mon"),
             f.max("dest_sold_to_name").alias("dest_sold_to_name_mon"),
             f.max("carr_desc").alias("carr_desc_mon"),
             f.max("origin_code").alias("origin_code"),
             f.max("shpmt_cnt_sum").alias("shpmt_cnt"),
             f.max("shpmt_on_time_cnt_sum").alias("shpmt_on_time_cnt"),
             f.max("measrbl_shpmt_cnt_sum").alias("measrbl_shpmt_cnt"),
             f.max("parent_carr_name").alias("parent_carr_name"),
             f.avg("cdot_ontime_cnt").alias("cdot_ontime_cnt")) \
        .withColumnRenamed("lane_name", "lane_key").drop("carr_desc") \
        .drop("origin_zone_ship_from_code").drop("dest_ship_from_code") \
        .drop("origin_code").drop("ship_to_party_code")\
        .drop("ship_to_party_desc_mon").drop("dest_sold_to_name_mon") \
        .drop("carr_desc_mon")

    logging.info("Joining data (1) has finished.")
    utils.manageOutput(logging, spark_session, netw_scorec1_df, 0, "netw_scorec1_df", target_db_name,
                       staging_location, debug_mode_ind, "_{}{}".format(target_db_name, debug_postfix))

    logging.info("Joining data (2).")

    netw_scorec2_df = tac_tender_pg_summary_df\
        .join(netw_scorec1_df, ["week_year_val", "lane_key", "forward_agent_id", "service_tms_code"],
              'left_outer')\

    logging.info("Joining data (2) has finished.")
    utils.manageOutput(logging, spark_session, netw_scorec2_df, 0, "netw_scorec2_df", target_db_name,
                       staging_location, debug_mode_ind, "_{}{}".format(target_db_name, debug_postfix))

    logging.info("Joining data (3).")

    netw_scorec3_df = lot_data_df.drop("load_id").drop("actual_ship_date").drop("freight_type_val")\
        .groupBy("carr_num", "dest_ship_from_code", "ship_point_code", 
                 "actual_service_tms_code", "lane_name", "week_year_val") \
        .agg(f.max("carr_desc").alias("carr_desc_lot"),
             f.max("dest_zone_val").alias("dest_zone_val_lot"),
             f.max("origin_zone_code").alias("origin_zone_code_lot"),
             f.max("origin_zone_ship_from_code").alias("origin_zone_ship_from_code_lot"),
             f.max("ship_to_party_code").alias("ship_to_party_code_lot"),
             f.max("ship_to_party_desc").alias("ship_to_party_desc_lot"),
             f.sum("lot_otd_cnt").alias("lot_otd_count"),
             f.sum("lot_tat_late_counter_val").alias("lot_tat_late_counter_val"),
             f.sum("lot_cust_failure_cnt").alias("lot_customer_failure_cnt"),
             f.sum("pg_failure_cnt").alias("pg_failure_cnt"),
             f.sum("carr_failure_cnt").alias("carr_failure_cnt"),
             f.sum("others_failure_cnt").alias("others_failure_cnt"),
             f.max("tolrnc_sot_val").alias("tolrnc_sot_val")) \
        .withColumnRenamed("lane_name", "lane_key") \
        .withColumnRenamed("actual_service_tms_code", "service_tms_code") \
        .withColumnRenamed("carr_num", "forward_agent_id") \
        .drop("dest_ship_from_code").drop("ship_point_code") \
        .drop("carr_desc_lot").drop("dest_zone_val_lot") \
        .drop("origin_zone_code_lot").drop("origin_zone_ship_from_code_lot") \
        .drop("ship_to_party_code_lot").drop("ship_to_party_desc_lot") \

    logging.info("Joining data (3) has finished.")
    utils.manageOutput(logging, spark_session, netw_scorec3_df, 0, "netw_scorec3_df", target_db_name,
                       staging_location, debug_mode_ind, "_{}{}".format(target_db_name, debug_postfix))

    logging.info("Joining data (4).")

    netw_scorec4_df = netw_scorec2_df\
        .join(netw_scorec3_df, ["week_year_val", "lane_key", "forward_agent_id", "service_tms_code"],
              'left_outer')\

    logging.info("Joining data (4) has finished.")
    utils.manageOutput(logging, spark_session, netw_scorec4_df, 0, "netw_scorec4_df", target_db_name,
                       staging_location, debug_mode_ind, "_{}{}".format(target_db_name, debug_postfix))

    logging.info("Joining data (5).")

    netw_scorec5_df = vfr_data_df.drop("load_id").drop("shpmt_start_date")\
        .groupBy("carrier_id", "tms_service_code", "origin_zone_ship_from_code",
        "dest_loc_code", "lane_name", "week_year_val") \
        .agg(f.max("carr_desc").alias("carr_desc_vfr"),
             f.max("origin_loc_id").alias("origin_loc_id_vfr"),
             f.max("dest_ship_from_code").alias("dest_ship_from_code_vfr"),
             f.sum("su_per_load_cnt").alias("su_per_load_cnt"),
             f.sum("plan_gross_weight_qty").alias("plan_gross_weight_qty"),
             f.sum("plan_net_weight_qty").alias("plan_net_weight_qty"),
             f.sum("shipped_gross_weight_qty").alias("shipped_gross_weight_qty"),
             f.sum("shipped_net_weight_qty").alias("shipped_net_weight_qty"),
             f.sum("plan_gross_vol_qty").alias("plan_gross_vol_qty"),
             f.sum("plan_net_vol_qty").alias("plan_net_vol_qty"),
             f.sum("shipped_gross_vol_qty").alias("shipped_gross_vol_qty"),
             f.sum("shipped_net_vol_qty").alias("shipped_net_vol_qty"),
             f.sum("max_weight_qty").alias("max_weight_qty"),
             f.sum("max_vol_trans_mgmt_sys_qty").alias("max_vol_trans_mgmt_sys_qty"),
             f.avg("floor_position_qty").alias("floor_position_qty"),
             f.avg("max_pallet_tms_trans_type_qty").alias("max_pallet_tms_trans_type_qty"),
             f.sum("cut_impact_rate").alias("cut_impact_rate"),
             f.sum("drf_last_truck_amt").alias("drf_last_truck_amt"),
             f.sum("glb_segment_impact_cat_ld_amt").alias("glb_segment_impact_cat_ld_amt"),
             f.sum("hopade_amt").alias("hopade_amt"),
             f.sum("max_orders_incrmtl_amt").alias("max_orders_incrmtl_amt"),
             f.sum("max_orders_non_drp_amt").alias("max_orders_non_drp_amt"),
             f.sum("total_vf_optny_amt").alias("total_vf_optny_amt")) \
        .withColumnRenamed("lane_name", "lane_key") \
        .withColumnRenamed("tms_service_code", "service_tms_code") \
        .withColumnRenamed("carrier_id", "forward_agent_id") \
        .drop("origin_zone_ship_from_code").drop("dest_loc_code") \
        .drop("carr_desc_vfr").drop("origin_loc_id_vfr") \
        .drop("dest_ship_from_code_vfr")\


    logging.info("Joining data (5) has finished.")
    utils.manageOutput(logging, spark_session, netw_scorec5_df, 0, "netw_scorec5_df", target_db_name,
                       staging_location, debug_mode_ind, "_{}{}".format(target_db_name, debug_postfix))

    logging.info("Joining data (6).")

    netw_scorec6_df = netw_scorec4_df\
        .join(netw_scorec5_df, ["week_year_val", "lane_key", "forward_agent_id", "service_tms_code"],
              'left_outer')\
        .withColumnRenamed("lane", "lane_name") \
        .drop("lane_key").drop("origin_loc_id_vfr") \

    logging.info("Joining data (6) has finished.")
    utils.manageOutput(logging, spark_session, netw_scorec6_df, 0, "netw_scorec6_df", target_db_name,
                       staging_location, debug_mode_ind, "_{}{}".format(target_db_name, debug_postfix))


    return netw_scorec6_df


def load_network_scorecard(logging, config_module, debug_mode_ind, debug_postfix):
    ''' Load the network_scorecard tables'''

    params = utils.ConfParams.build_from_module(logging, config_module, debug_mode_ind, debug_postfix)
    #spark_params = utils.ConfParams.build_from_module(logging, config_spark_module, debug_mode_ind, debug_postfix)
    spark_conf = list(set().union(
        params.SPARK_GLOBAL_PARAMS,
        params.SPARK_PROD_OTD_PARAMS)
        )
    spark_session = utils.get_spark_session(logging, 'tfx_netw', params.SPARK_MASTER, spark_conf)

    #Remove debug tables (if they are)
    utils.removeDebugTables(logging, spark_session, params.TARGET_DB_NAME, debug_mode_ind, debug_postfix)

    # Get target table column list
    target_table_cols = spark_session.table('{}.weekly_network_sccrd_star'.format(params.TARGET_DB_NAME))\
        .schema.fieldNames()

    weekly_network_sccrd_star_df = \
        get_weekly_network_sccrd_star(logging, spark_session, params.TRANS_VB_DB_NAME, params.TARGET_DB_NAME,
                                   params.STAGING_LOCATION, debug_mode_ind, debug_postfix).select(target_table_cols)

    logging.info("Inserting data into a table (overwriting old data)")

    weekly_network_sccrd_star_df.write.insertInto(
        tableName='{}.{}'.format(params.TARGET_DB_NAME, 'weekly_network_sccrd_star'),
        overwrite=True)
    logging.info("Loading {}.weekly_network_sccrd_star table has finished".format(params.TARGET_DB_NAME))
